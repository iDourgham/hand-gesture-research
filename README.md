# **Hand Gesture Classification Using MediaPipe Landmarks from the HaGRID Dataset**

## **Overview**
This project focuses on classifying hand gestures using landmark data extracted by **MediaPipe** from the **HaGRID (Hand Gesture Recognition Image Dataset)**. The goal is to train a machine learning model capable of accurately classifying gestures based on the **x, y, z** coordinates of hand landmarks.

Students will gain experience in:  
âœ… Data preprocessing and visualization  
âœ… Model training and evaluation  
âœ… Documentation and presentation of results  

---

## **Dataset Details**
- The **HaGRID** dataset contains **18 hand gesture classes**.
- Each gesture is represented by **21 hand landmarks** (x, y, z) extracted using **MediaPipe**.
- The dataset is provided as a **CSV file**, where each row contains:  
  - **Landmark coordinates (x, y, z) for 21 keypoints**  
  - **Gesture label**  

### **Preprocessing Notes:**
- Hand landmarks vary in **scale and position**. To standardize them:  
  - **Recenter (x, y) coordinates** to make the wrist point the origin.  
  - **Normalize landmarks** by dividing by the mid-finger tip position.  
- The **z-location** does not require further processing.  
- **Stabilize output predictions** by applying a **moving mode filter** over a window.  

---

## **Project Deliverables**
### âœ… **1. Google Colab Notebook**
A well-documented notebook uploaded to **GitHub** that includes:  
- ðŸ“Œ **Data Loading:** Importing the CSV file and understanding its structure.  
- ðŸ“Œ **Data Visualization:** Plotting sample hand landmarks.  
- ðŸ“Œ **Preprocessing:** Cleaning and normalizing the data, handling missing values, and splitting into train/test sets.  
- ðŸ“Œ **Model Training:** Implementing and comparing at least **three ML models** (e.g., **Random Forest, SVM**).  
- ðŸ“Œ **Evaluation:** Reporting **accuracy, precision, recall, and F1-score**.  
- ðŸ“Œ **Conclusion:** Summarizing results and selecting the best model.  

### âœ… **2. Output Video**
ðŸŽ¥ A short video demonstrating the trained model in action:  
- **MediaPipe extracts hand landmarks** from each frame.  
- **Predictions are displayed** in real-time.  
- The video must be uploaded to **Google Drive** with a **public link**.  

ðŸ“Œ **Watch the demonstration video here:** [Hand Gesture Classification Demo](https://drive.google.com/file/d/1wyZIUwyqijr3Z_Y1rER77UWRb4dP_SCM/view?usp=sharing)  

---

## **Evaluation Criteria**
âœ… **Code Quality** â€“ Clean, well-documented, and reproducible.  
âœ… **Model Performance** â€“ Accuracy and robustness of trained models.  
âœ… **Visualization** â€“ Clear representation of data and results.  

---

## Setup Instructions

### 1. Create the environment

You can set up the required environment either with Conda or pip.

#### Using Conda (recommended):

```bash
conda env create -f environment.yml
conda activate mediapipe_env

---

## Contributors
- **Mohamed Mohy** ([GitHub Profile](https://github.com/iDourgham))  
                   ([LinkedIn Profile](https://www.linkedin.com/in/eng-m-mohy/))


